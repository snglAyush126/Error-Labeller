# -*- coding: utf-8 -*-
"""BERTopic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V-VlYsS3NJE05-u8MLvLCD1P_sYg-7HL
"""

import pandas as pd

pip install bertopic

from bertopic import BERTopic

import numpy as np
import pandas as pd
import random as rn
import re
import nltk
import os
import io
import umap
import hdbscan
import spacy

nlp = spacy.load("en_core_web_sm")

def preprocess_text1(text):
        # Lowercase text
        text = text.lower()

        # Remove numbers
        text = re.sub(r"\d+", "", text)

        # Remove special characters
        text = re.sub(r"[^\w\s]", " ", text)

        # Remove emails
        text = re.sub(r"\S+@\S+", "", text)

        # Remove weblinks
        text = re.sub(r"http\S+|www\S+|https\S+", "", text)

        text = re.sub(r'\b\w{1,10}\b', '', text)

        text = re.sub(r"\s+", " ", text)
        # Tokenize and remove stop words
        doc = nlp(text)
        tokens = [token.text for token in doc if not token.is_stop]

        # Join the tokens back into a string
        preprocessed_text = " ".join(tokens)

        return preprocessed_text

from nltk.corpus import stopwords
from nltk import WordNetLemmatizer
nltk.download('stopwords')
from nltk.stem import PorterStemmer

stop = set(stopwords.words('english'))
stemmer = PorterStemmer()
lemma = WordNetLemmatizer()

def remove_stopwords(text):
    text = [word.lower() for word in text.split() if word.lower() not in stop ]
    return ' '.join(text)

from nltk.stem import SnowballStemmer

def Stemming(text):
    stem = []
    stopword = stopwords.words('english')
    snowball_stemmer = SnowballStemmer('english')
    word_tokens = nltk.word_tokenize(text)
    stemmed_word = [ snowball_stemmer.stem(word) for word in word_tokens ]
    stem = ' '.join(stemmed_word)
    return stem

pip install -U sentence-transformers

from sentence_transformers import SentenceTransformer
import tensorflow as tf

from google.colab import files
uploaded = files.upload()

df = pd.read_csv(io.BytesIO(uploaded['problem_dataset.csv']))
df.head(32)

stacktrace = df['stacktrace'].tolist()

docs = []
for doc in stacktrace:
  cleaned = preprocess_text1(doc[:200])
  docs.append(cleaned)

sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = sentence_model.encode(docs, show_progress_bar=False)

from functools import partial

from hyperopt import fmin, tpe, hp, STATUS_OK, space_eval, Trials

import random

def generate_clusters(
    message_embeddings,
    n_neighbors,
    n_components,
    min_cluster_size,
    min_samples=None,
    random_state=None,
):

    umap_embeddings = (
        umap.UMAP(
            n_neighbors=n_neighbors,
            n_components=n_components,
            metric="cosine",
            random_state=random_state,
        )
    ).fit_transform(message_embeddings)

    clusters = hdbscan.HDBSCAN(
        min_cluster_size=min_cluster_size,
        min_samples=min_samples,
        metric="euclidean",
        gen_min_span_tree=True,
        cluster_selection_method="eom",
    ).fit(umap_embeddings)

    return clusters

def score_clusters(clusters, prob_thresh0ld=0.05):
    cluster_labels = clusters.labels_

    label_count = len(np.unique(cluster_labels))
    total_num = len(clusters.labels_)

    cost = np.count_nonzero(clusters.probabilities_ < prob_thresh0ld) / total_num

    return label_count, cost

def objective(params, embeddings, label_lower, label_upper):
    clusters = generate_clusters(
        embeddings,
        n_neighbors=params["n_neighbors"],
        n_components=params["n_components"],
        min_cluster_size=params["min_cluster_size"],
        min_samples = params["min_samples"],
        random_state=params["random_state"],
    )

    label_count, cost = score_clusters(clusters, prob_thresh0ld=0.05)

    if (label_count < label_lower) | (label_count > label_upper):
        penalty = 1.0
    else:
        penalty = 0

    loss = cost + penalty

    return {"loss": loss, "label_count": label_count, "status": STATUS_OK}

def bayesian_search(embeddings, space, label_lower, label_upper, max_evals=100):

    trials = Trials()
    fmin_objective = partial(
        objective,
        embeddings=embeddings,
        label_lower=label_lower,
        label_upper=label_upper,
    )

    best = fmin(
        fmin_objective,
        space=space,
        algo=tpe.suggest,
        max_evals=max_evals,
        trials=trials,
    )

    best_params = space_eval(space, best)
    print("best:")
    print(best_params)
    print(f"label count: {trials.best_trial['result']['label_count']}")

    best_clusters = generate_clusters(
        embeddings,
        n_neighbors=best_params["n_neighbors"],
        n_components=best_params["n_components"],
        min_cluster_size=best_params["min_cluster_size"],
        min_samples = best_params["min_samples"],
        random_state=best_params["random_state"],
    )

    return best_params, best_clusters, trials

hspace = {
    "n_neighbors": hp.choice('n_neighbors', range(2,5)),
    "n_components": hp.choice('n_components', range(1,5)),
    "min_cluster_size": hp.choice('min_cluster_size', range(2,5)),
    "min_samples" : hp.choice('min_samples' , range(1,5) ),
    "random_state": 42
}
label_lower = 10
label_upper = 100
max_evals = 25 # change it to 50 or 100 for extra steps as you wish.

best_params_use, best_clusters_use, trials_use = bayesian_search(embeddings,
                                                                 space=hspace,
                                                                 label_lower=label_lower,
                                                                 label_upper=label_upper,
                                                                 max_evals=max_evals)

data_clustered = pd.DataFrame(data = list(zip(docs, best_clusters_use.labels_ )) , columns=['text', 'label_st1' ] )

data_clustered.head()

example_category = data_clustered[data_clustered['label_st1'] == 35].reset_index(drop = True)
example_category.head()

from collections import Counter

def get_group(df, category_col, category ):
    single_category = df[df[category_col] == category ].reset_index(drop = True)
    return single_category

def extract_labels(category_doc, print_word_counts = False):


    error_l = []
    exception_l = []


    for i in range(len(category_doc)):
        doc = nlp(category_doc[i])
        for token in doc:
            if (len(str(token).strip()) >5 ) and (str(token)[-5:].lower() == "error"):
              error_l.append(str(token).lower())
            elif (len(str(token).strip()) >9) and (str(token)[-9:].lower() == "exception"):
              exception_l.append(str(token).lower())



    x1 = Counter(error_l)
    x2 = Counter(exception_l)

    w1 = ""
    w2 = ""

    if len(list(x1.keys()))>0:
      w1 = list(x1.keys())[0]

    if len(list(x2.keys()))>0:
      w2 = list(x2.keys())[0]

    label = w1+" "+w2


    return label

def apply_and_summarize_labels(df, category_col ):

    numerical_labels = df[category_col].unique()

    label_dict = {}

    for label in numerical_labels:
        current_category = list(get_group(data_clustered, category_col, label )['text']) #list of all the documents text having label_st1 = label

        label_dict[label] = extract_labels(current_category)

    summary_df = (df.groupby(category_col)['text'].count()
                  .reset_index()
                  .rename(columns={'text' : 'count' })
                  .sort_values('count', ascending = False )
                  )

    summary_df['label'] = summary_df.apply(lambda x: label_dict[x[category_col]] , axis = 1 )

    return summary_df

cluster_summary = apply_and_summarize_labels(data_clustered, 'label_st1' )
cluster_summary

test_keys = cluster_summary['label_st1'].to_list()
test_values = cluster_summary['label'].to_list()

col1 = data_clustered['label_st1'].to_list()

res = {test_keys[i]: test_values[i] for i in range(len(test_keys))}

res2 = []

for label_no in col1:
  res2.append(res[label_no])

data_clustered['label'] = pd.DataFrame(res2)
df['label_generated'] = pd.DataFrame(res2)

df.head(30)

from sklearn.metrics import accuracy_score
accuracy_score(df.original_label, df.label_generated)

df3 = df[['ID' , 'stacktrace' , 'original_label']]
df3.head()

df.head()

df3.to_csv('problem_dataset.csv')

from google.colab import drive
drive.mount('/content/drive')

df3.to_csv('/content/drive/My Drive/problem_dataset.csv', index=False)

df.to_csv('/content/drive/My Drive/Labels_generated.csv', index=False)

