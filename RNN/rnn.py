# -*- coding: utf-8 -*-
"""RNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IECu274sIhutxS2btb6Mi4BiK8d3ElEG
"""

import tensorflow as tf

from tensorflow import keras

import pandas as pd
import string
import re

import nltk

from nltk.stem.porter import PorterStemmer
from nltk.stem.lancaster import LancasterStemmer
from nltk.stem import WordNetLemmatizer
from nltk.stem import SnowballStemmer
from nltk.corpus import stopwords

nltk.download('stopwords' , 'punkt')

def text_preprocess(messages):
  ps = PorterStemmer()
  lemmatizer = WordNetLemmatizer()
  corpus=[]
  for i in range(0 , len(messages)):
    review = re.sub('[^a-zA-Z]' , ' ' , messages[i])
    review = review.lower()
    review = review.split()
    review = [lemmatizer.lemmatize(word) for word in review if not word in stopwords.words('english')]
    review = ' '.join(review);
    corpus.append(review)
  return corpus

import itertools
import numpy as np
np.random.seed(1337)
from gensim.models import Word2Vec
from keras.models import Model
from keras.layers import Dense, Dropout, LSTM, Input, BatchNormalization, Bidirectional
from keras.utils import np_utils
from keras.optimizers import RMSprop
from sklearn.utils import shuffle

min_word_frequency_word2vec = 3
embed_size_word2vec = 100
context_window_word2vec = 5


max_sentence_len = 50
min_sentence_length = 2
batch_size = 32

from google.colab import files
uploaded = files.upload()

import io

data = pd.read_csv(io.BytesIO(uploaded['problem_dataset.csv']))
data.head()

import nltk
nltk.download('stopwords')

nltk.download('wordnet')

data['stacktrace'] = text_preprocess(data['stacktrace'])

class_to_predict = 'original_label'

data.head()

num_records = len(data)
  data_train = data[:int(0.85 * num_records)]
  data_test = data[int(0.85 * num_records):]

train_data = [x[0] for x in data_train[['stacktrace']].to_records(index=False)]
  train_labels = [x[0] for x in data_train[[class_to_predict]].to_records(index=False)]
  unique_train_label = list(set(train_labels))

lable_id = {}
for i,label_ in enumerate(unique_train_label):
  lable_id[i] = label_

lable_id[-1]

len(unique_train_label)

test_data = [x[0] for x in data_test[['stacktrace']].to_records(index=False)]
  test_labels = [x[0] for x in data_test[[class_to_predict]].to_records(index=False)]

# Tokenize data
  train_data = [text.split() for text in train_data] # TODO(Vladimir) - try nltk tokenize here
  test_data = [text.split() for text in test_data]
  all_data = train_data + test_data

print('Data examples')
  print(all_data[0])

wordvec_model = Word2Vec(all_data, min_count=min_word_frequency_word2vec,
                           vector_size=embed_size_word2vec, window=context_window_word2vec)
  vocabulary = wordvec_model.wv
  vocab_size = len(vocabulary)

len(vocabulary)

X_train = np.empty(shape=[len(train_data), max_sentence_len, embed_size_word2vec],
                     dtype='float32')
  Y_train = np.empty(shape=[len(train_labels), 1], dtype='int32')

for j, curr_row in enumerate(train_data):
    if j % 100 == 0:
      print('Building X_train j = ' + str(j))
    sequence_cnt = 0
    for item in curr_row:
      if item in vocabulary:
        X_train[j, sequence_cnt, :] = wordvec_model.wv[item]
        sequence_cnt = sequence_cnt + 1
        if sequence_cnt == max_sentence_len - 1:
          break
    for k in range(sequence_cnt, max_sentence_len):
      X_train[j, k, :] = np.zeros((1, embed_size_word2vec))
    Y_train[j, 0] = unique_train_label.index(train_labels[j])

X_test = np.empty(shape=[len(test_data), max_sentence_len, embed_size_word2vec],
                    dtype='float32')
  Y_test = np.empty(shape=[len(test_labels), 1], dtype='int32')

for j, curr_row in enumerate(test_data):
    if j % 100 == 0:
      print('Building X_test j = ' + str(j))
    sequence_cnt = 0
    for item in curr_row:
      if item in vocabulary:
        X_test[j, sequence_cnt, :] = wordvec_model.wv[item]
        sequence_cnt = sequence_cnt + 1
        if sequence_cnt == max_sentence_len - 1:
          break
    for k in range(sequence_cnt, max_sentence_len):
      X_test[j, k, :] = np.zeros((1, embed_size_word2vec))
    try:
      Y_test[j, 0] = unique_train_label.index(test_labels[j])
    except:
      Y_test[j,0] = -1

y_train = np_utils.to_categorical(Y_train, len(unique_train_label))

y_train[0].shape

sequence = Input(shape=(max_sentence_len, embed_size_word2vec), dtype='float32')
  lstm = Bidirectional(LSTM(1024))(sequence)
  after_dp = Dropout(0.5)(lstm)
  output = Dense(len(unique_train_label), activation='softmax')(after_dp)
  model = Model(inputs=sequence, outputs=output)
  rms = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08)
  model.compile(loss='categorical_crossentropy', optimizer=rms, metrics=['accuracy'])

tf.config.run_functions_eagerly(True)

hist = model.fit(X_train, y_train,
                   batch_size=batch_size,
                   epochs=10)

train_result = hist.history
  print(train_result)

train_prediction = model.predict(X_train)

len(wordvec_model.wv["io"])

total_train_correct = 0

  pred = []


  for j, ll in enumerate(train_prediction):
    label_predicted = lable_id[np.argmax(ll)]
    pred.append(label_predicted)
    if np.argmax(ll) == Y_train[j]:
      total_train_correct += 1


  print('Train accuracy:', total_train_correct * 1.0 / len(train_prediction))

test_prediction = model.predict(X_test)

total_test_correct = 0
  labels = []
  predicted = []
  for j, ll in enumerate(test_prediction):
    predicted.append(np.argmax(ll))
    labels.append(Y_test[j])
    if np.argmax(ll) == Y_test[j]:
      total_test_correct += 1

  print('Test accuracy:', total_test_correct * 1.0 / len(test_prediction))

